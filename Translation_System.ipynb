{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiBpxR8r+B9LuFrgRr00i4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssef893/Translation-Model/blob/main/Translation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import important libraries\n"
      ],
      "metadata": {
        "id": "CNU4AC6ZCjXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "3EzdyyVT71vl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_1NtnJrAXN0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array"
      ],
      "metadata": {
        "id": "IJ8-yQimCbch"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Text Data\n",
        "Take a look at the raw data and note what you see that we might need to handle in a data cleaning operation.\n",
        "\n",
        "For example, here are some observations I note from reviewing the raw data:\n",
        "\n",
        "1- There is punctuation.\n",
        "2- The text contains uppercase and lowercase.\n",
        "3- There are special characters in the German.\n",
        "4- There are duplicate phrases in English with different translations  in German.\n",
        "5- The file is ordered by sentence length with very long sentences toward the end of the file.\n",
        "\n",
        "A good text cleaning procedure may handle some or all of these observations.\n",
        "\n",
        "Data preparation is divided into two subsections:\n",
        "\n",
        "1- Clean Text\n",
        "2- Split Text"
      ],
      "metadata": {
        "id": "jr44ihOFDBg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Clean Text"
      ],
      "metadata": {
        "id": "mlMlHE3OCvcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n"
      ],
      "metadata": {
        "id": "MmtuGNyyCYNf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "filename = 'deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FHJAeFzCfS0",
        "outputId": "0dc38fcf-4a1f-4032-fc24-8c77d3cd8ee7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stop] => [stopp]\n",
            "[wait] => [warte]\n",
            "[hello] => [hallo]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[smile] => [lacheln]\n",
            "[cheers] => [zum wohl]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n",
            "[really] => [im ernst]\n",
            "[thanks] => [danke]\n",
            "[try it] => [versuchs]\n",
            "[why me] => [warum ich]\n",
            "[ask tom] => [frag tom]\n",
            "[ask tom] => [fragen sie tom]\n",
            "[ask tom] => [fragt tom]\n",
            "[be cool] => [entspann dich]\n",
            "[be fair] => [sei nicht ungerecht]\n",
            "[be fair] => [sei fair]\n",
            "[be nice] => [sei nett]\n",
            "[be nice] => [seien sie nett]\n",
            "[beat it] => [geh weg]\n",
            "[beat it] => [hau ab]\n",
            "[beat it] => [verschwinde]\n",
            "[beat it] => [verdufte]\n",
            "[beat it] => [mach dich fort]\n",
            "[beat it] => [zieh leine]\n",
            "[beat it] => [mach dich vom acker]\n",
            "[beat it] => [verzieh dich]\n",
            "[beat it] => [verkrumele dich]\n",
            "[beat it] => [troll dich]\n",
            "[beat it] => [zisch ab]\n",
            "[beat it] => [pack dich]\n",
            "[beat it] => [mach ne fliege]\n",
            "[beat it] => [schwirr ab]\n",
            "[beat it] => [mach die sause]\n",
            "[beat it] => [scher dich weg]\n",
            "[beat it] => [scher dich fort]\n",
            "[call me] => [ruf mich an]\n",
            "[come in] => [komm herein]\n",
            "[come in] => [herein]\n",
            "[come on] => [komm]\n",
            "[come on] => [kommt]\n",
            "[come on] => [mach schon]\n",
            "[come on] => [macht schon]\n",
            "[get out] => [raus]\n",
            "[go away] => [geh weg]\n",
            "[go away] => [hau ab]\n",
            "[go away] => [verschwinde]\n",
            "[go away] => [verdufte]\n",
            "[go away] => [mach dich fort]\n",
            "[go away] => [zieh leine]\n",
            "[go away] => [mach dich vom acker]\n",
            "[go away] => [verzieh dich]\n",
            "[go away] => [verkrumele dich]\n",
            "[go away] => [troll dich]\n",
            "[go away] => [zisch ab]\n",
            "[go away] => [pack dich]\n",
            "[go away] => [mach ne fliege]\n",
            "[go away] => [schwirr ab]\n",
            "[go away] => [mach die sause]\n",
            "[go away] => [scher dich weg]\n",
            "[go away] => [scher dich fort]\n",
            "[go away] => [geh weg]\n",
            "[go away] => [verpiss dich]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Split Text\n",
        "The clean data contains a little over 150,000 phrase pairs and some of the pairs toward the end of the file are very long.\n",
        "\n",
        "This is a good number of examples for developing a small translation model. The complexity of the model increases with the number of examples, length of phrases, and size of the vocabulary.\n",
        "\n",
        "Although we have a good dataset for modeling translation, we will simplify the problem slightly to dramatically reduce the size of the model required, and in turn the training time required to fit the model.\n",
        "\n",
        "We will simplify the problem by reducing the dataset to the first 30,000 examples in the file; these will be the shortest phrases in the dataset.\n",
        "\n",
        "Further, we will then stake the first 20,000 of those as examples for training and the remaining 10,000 examples to test the fit model."
      ],
      "metadata": {
        "id": "eA1OZ9IVCg4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 30000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:20000], dataset[20000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lwS_91BEzDg",
        "outputId": "1ded47e6-8bed-4081-aeb2-1c39def08266"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Neural Translation Model"
      ],
      "metadata": {
        "id": "YGS4PXdSFES2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This involves both loading and preparing the clean text data ready for modeling and defining and training the model on the prepared data."
      ],
      "metadata": {
        "id": "xJxFIMQ8FRDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "q2DeD_Xt7yjd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvMhj8M57pWz",
        "outputId": "1775cb63-de0d-45c4-dfb3-3de4a76e3987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 4937\n",
            "English Max Length: 7\n",
            "German Vocabulary Size: 8097\n",
            "German Max Length: 11\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 11, 256)           2072832   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVect  (None, 7, 256)           0         \n",
            " or)                                                             \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 7, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 7, 4937)          1268809   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,392,265\n",
            "Trainable params: 4,392,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.35820, saving model to model.h5\n",
            "313/313 - 15s - loss: 3.8562 - val_loss: 3.3582 - 15s/epoch - 48ms/step\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 3.35820 to 3.22026, saving model to model.h5\n",
            "313/313 - 6s - loss: 3.2466 - val_loss: 3.2203 - 6s/epoch - 19ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 3.22026 to 3.10365, saving model to model.h5\n",
            "313/313 - 6s - loss: 3.0944 - val_loss: 3.1037 - 6s/epoch - 18ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.10365 to 2.95658, saving model to model.h5\n",
            "313/313 - 6s - loss: 2.9315 - val_loss: 2.9566 - 6s/epoch - 18ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 2.95658 to 2.78827, saving model to model.h5\n",
            "313/313 - 6s - loss: 2.7420 - val_loss: 2.7883 - 6s/epoch - 18ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 2.78827 to 2.63165, saving model to model.h5\n",
            "313/313 - 6s - loss: 2.5470 - val_loss: 2.6317 - 6s/epoch - 19ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 2.63165 to 2.46340, saving model to model.h5\n",
            "313/313 - 6s - loss: 2.3310 - val_loss: 2.4634 - 6s/epoch - 20ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 2.46340 to 2.33807, saving model to model.h5\n",
            "313/313 - 6s - loss: 2.1319 - val_loss: 2.3381 - 6s/epoch - 18ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 2.33807 to 2.21992, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.9547 - val_loss: 2.2199 - 6s/epoch - 18ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.21992 to 2.12518, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.7942 - val_loss: 2.1252 - 6s/epoch - 18ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 2.12518 to 2.05493, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.6427 - val_loss: 2.0549 - 6s/epoch - 18ms/step\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 2.05493 to 1.97919, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.5033 - val_loss: 1.9792 - 6s/epoch - 18ms/step\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 1.97919 to 1.93024, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.3710 - val_loss: 1.9302 - 6s/epoch - 18ms/step\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 1.93024 to 1.88059, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.2523 - val_loss: 1.8806 - 6s/epoch - 18ms/step\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 1.88059 to 1.84484, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.1359 - val_loss: 1.8448 - 6s/epoch - 18ms/step\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 1.84484 to 1.80920, saving model to model.h5\n",
            "313/313 - 6s - loss: 1.0299 - val_loss: 1.8092 - 6s/epoch - 18ms/step\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 1.80920 to 1.77914, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.9309 - val_loss: 1.7791 - 6s/epoch - 18ms/step\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 1.77914 to 1.75405, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.8419 - val_loss: 1.7540 - 6s/epoch - 18ms/step\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 1.75405 to 1.74103, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.7588 - val_loss: 1.7410 - 6s/epoch - 18ms/step\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 1.74103 to 1.72247, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.6862 - val_loss: 1.7225 - 6s/epoch - 18ms/step\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss did not improve from 1.72247\n",
            "313/313 - 6s - loss: 0.6193 - val_loss: 1.7234 - 6s/epoch - 18ms/step\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 1.72247 to 1.71783, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.5568 - val_loss: 1.7178 - 6s/epoch - 18ms/step\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 1.71783 to 1.70795, saving model to model.h5\n",
            "313/313 - 6s - loss: 0.5039 - val_loss: 1.7079 - 6s/epoch - 18ms/step\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.4566 - val_loss: 1.7177 - 6s/epoch - 18ms/step\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.4141 - val_loss: 1.7186 - 6s/epoch - 18ms/step\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.3774 - val_loss: 1.7385 - 6s/epoch - 18ms/step\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.3441 - val_loss: 1.7359 - 6s/epoch - 18ms/step\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.3126 - val_loss: 1.7447 - 6s/epoch - 18ms/step\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.2853 - val_loss: 1.7520 - 6s/epoch - 18ms/step\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.70795\n",
            "313/313 - 6s - loss: 0.2605 - val_loss: 1.7630 - 6s/epoch - 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f65177be850>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Neural Translation Model"
      ],
      "metadata": {
        "id": "1tceBaqZGUTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wVY79VSGV-k",
        "outputId": "c431f1b6-6676-4bc3-d793-8c605eed1795"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[haben sie decken], target=[do you have blankets], predicted=[do you have blankets]\n",
            "src=[dieser junge ist intelligent], target=[that boy is smart], predicted=[this boy is]\n",
            "src=[ich wurde sehr gerne hierbleiben], target=[id love to stay], predicted=[id love to stay]\n",
            "src=[hast du thunfisch], target=[do you have tuna fish], predicted=[do you have tuna fish]\n",
            "src=[sie kann nicht schwimmen], target=[she cant swim], predicted=[she cannot swim]\n",
            "src=[vierzig leute kamen], target=[forty people attended], predicted=[forty people attended]\n",
            "src=[ich wurde gerne gehen], target=[id like to go], predicted=[id like to go]\n",
            "src=[ich hasse diese musik], target=[i hate this music], predicted=[i hate this music]\n",
            "src=[bitte setzen sie sich], target=[please take a seat], predicted=[please sit a]\n",
            "src=[ich habe tom gerade erst kennengelernt], target=[i only just met tom], predicted=[i just just just tom]\n",
            "BLEU-1: 0.840214\n",
            "BLEU-2: 0.771045\n",
            "BLEU-3: 0.722561\n",
            "BLEU-4: 0.582174\n",
            "test\n",
            "src=[die tur ist auf], target=[the door is open], predicted=[the door closed]\n",
            "src=[ich sah jemanden], target=[i saw somebody], predicted=[i saw someone]\n",
            "src=[er arbeitet fast nie], target=[he hardly ever works], predicted=[he never never]\n",
            "src=[ich konnte mich nicht bewegen], target=[i couldnt move], predicted=[i cant get me]\n",
            "src=[fugt mehr wasser hinzu], target=[add more water to it], predicted=[add more water to]\n",
            "src=[wurde ich gesehen als ich ging], target=[was i seen leaving], predicted=[did i miss a]\n",
            "src=[er sieht oft fern], target=[he often watches tv], predicted=[he knows in in]\n",
            "src=[mach keine szene], target=[dont make a scene], predicted=[dont make a]\n",
            "src=[er fing zu weinen an], target=[he began to cry], predicted=[he started to]\n",
            "src=[dieser tee schmeckt gut], target=[this tea tastes good], predicted=[this tea well well]\n",
            "BLEU-1: 0.549659\n",
            "BLEU-2: 0.418792\n",
            "BLEU-3: 0.360866\n",
            "BLEU-4: 0.235495\n"
          ]
        }
      ]
    }
  ]
}